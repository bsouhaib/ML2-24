{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OTDHcfN1Xd"
      },
      "source": [
        "# Machine Learning II 2023-2024 - UMONS\n",
        "# Variational autoencoders\n",
        "\n",
        "In this lab, we will implement variational autoencoders.\n",
        "\n",
        "The plan of the lab is as follows:\n",
        "1. The `Distribution` objects are introduced. This will allow us to conveniently work with distributions in PyTorch.\n",
        "2. We implement the encoder and decoder of the variational autoencoder.\n",
        "3. We implement the loss of the variational autoencoder.\n",
        "4. Optional experiments are provided at the end, for those that are interested.\n",
        "\n",
        "The parts that have to be completed are indicated using `TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMNYnclVaMlP"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDNcWqRCaMlQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as D\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMyLmMwMN1Xg"
      },
      "source": [
        "## PyTorch distributions\n",
        "\n",
        "Many `Distribution` objects are available in [PyTorch](https://pytorch.org/docs/stable/distributions.html).\n",
        "In this short tutorial, we will explore the `Normal` distribution and how shapes of `Distribution` objects are handled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-2OUlQ-N1Xg"
      },
      "outputs": [],
      "source": [
        "# We create a normal distribution with mean 0 and standard deviation 1.\n",
        "dist = D.Normal(0, 1)\n",
        "print(f'Mean: {dist.loc}, standard deviation: {dist.scale}')\n",
        "\n",
        "fig, axis = plt.subplots()\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "# We plot the probability density function (PDF) using the `log_prob` method.\n",
        "axis.plot(x, dist.log_prob(x).exp(), label='PDF')\n",
        "# We plot the cumulative density function (CDF) using the `cdf` method.\n",
        "axis.plot(x, dist.cdf(x), label='CDF')\n",
        "# We plot a histogram of 1000 samples from the distribution.\n",
        "axis.hist(dist.sample((1000,)), density=True, alpha=0.5, label='Samples')\n",
        "axis.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDDQnTs6N1Xh"
      },
      "source": [
        "While PyTorch `Tensor` objects have a single `.shape`, `Distribution` objects have two shape attributes that are semantically different:\n",
        "- `event_shape`: The shape of a single event from the distribution.\n",
        "- `batch_shape`: The shape of a single sample from one or more distribution of the same family. As an example, we canâ€™t have a batch of a normal and Gamma distribution together, but we can have a batch of more than one normal distributions with different means and standard deviations.\n",
        "\n",
        "Additionally, a `sample_shape` argument can be given when sampling from `Distribution` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLWVMZDoN1Xh"
      },
      "outputs": [],
      "source": [
        "# We create 3 normal distributions with means -2, 0 and 2, and standard deviation 1.\n",
        "dist = D.Normal(torch.tensor([-2., 0., 2.]), torch.ones(3))\n",
        "# By default, `Normal` distributions have an empty `event_shape` because they are univariate.\n",
        "assert dist.batch_shape == (3,) and dist.event_shape == ()\n",
        "# The shape of a sample is `sample_shape + batch_shape + event_shape`.\n",
        "sample_shape = (2,)\n",
        "assert dist.sample(sample_shape).shape == (2, 3)\n",
        "# The shape returned by `log_prob` is `sample_shape + batch_shape`.\n",
        "assert dist.log_prob(torch.zeros(3)).shape == (3,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUSZjO_xN1Xh"
      },
      "source": [
        "To represent multivariate normal distributions with a diagonal covariance, we can use the `Independent` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O7KNCL5N1Xi"
      },
      "outputs": [],
      "source": [
        "dist = D.Normal(torch.tensor([-2., 0., 2.]), torch.ones(3))\n",
        "# The last dimension of the `batch_shape` is reinterpreted as the `event_shape`.\n",
        "dist = D.Independent(dist, 1)\n",
        "assert dist.batch_shape == () and dist.event_shape == (3,)\n",
        "# Samples keep the same dimension than before.\n",
        "sample_shape = (2,)\n",
        "assert dist.sample(sample_shape).shape == (2, 3)\n",
        "# The shape returned by `log_prob` is different because we compute the log PDF of a single multivariate distribution.\n",
        "assert dist.log_prob(torch.zeros(3)).shape == ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQMVkzzcN1Xi"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a `Normal` distribution with a `batch_shape` of `(4, 5)` and an `event_shape` of `(3,)`.\n",
        "# Fix the means and standard deviations at 0 and 1.\n",
        "# Then, sample from this distribution with a `sample_shape` of `(2,)`.\n",
        "# What is the shape of the sampled event?\n",
        "dist ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5l7u57jaMlR"
      },
      "source": [
        "## Constants and hyperparameters of the VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi-63Dy_aMlR"
      },
      "outputs": [],
      "source": [
        "height, width = 28, 28\n",
        "X_dim = height * width\n",
        "Z_dim = 64\n",
        "H_dim = 128\n",
        "\n",
        "num_epochs = 8\n",
        "batch_size = 128\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fTamOjvaMlR"
      },
      "source": [
        "## Load and plot the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in5CYH7yaMlR"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "     # We normalize the data to have zero mean and unit standard deviation using precomputed values.\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "     # We simplify the problem by transforming the gray images to binary images.\n",
        "    lambda x: x > 0,\n",
        "])\n",
        "train_data = MNIST('./data', download=True, train=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN9KejYUaMlS"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    grid = torchvision.utils.make_grid(images, nrow=16)\n",
        "    fig, axis = plt.subplots(dpi=150)\n",
        "    axis.imshow(grid[0], cmap='gray')\n",
        "    axis.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "first_batch = next(iter(train_loader))[0]\n",
        "plot_images(first_batch.float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EGoGglaMlT"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOcnDAvKaMlT"
      },
      "outputs": [],
      "source": [
        "# The encoder generates a distribution over the latent space from image samples.\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: implement the encoder model.\n",
        "        # It should be similar to the decoder model except that the last layers are different.\n",
        "        # The output dimension should be two times the latent dimension (Z_dim) because\n",
        "        # we need both the mean and the variance of the distribution.\n",
        "        self.model =\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We scale the values to the [-1, 1] interval.\n",
        "        x = (x - 0.5) * 2\n",
        "        # We split the output into two parts of equal size.\n",
        "        mu, rho = torch.chunk(self.model(x), 2, dim=1)\n",
        "        # TODO: Firstly, ensure that the standard deviation is positive using the `F.softplus` function.\n",
        "        std =\n",
        "        # TODO: Secondly, create a distribution over the latent space using the `Normal` and `Independent` classes.\n",
        "        # The returned distribution should be a multivariate normal of dimension `Z_dim` with diagonal covariance matrix.\n",
        "        return\n",
        "\n",
        "\n",
        "# The decoder generates a distribution over binary images from latent samples.\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # At the last layers, we have to use `nn.Sigmoid` to ensure that the output is\n",
        "        # a valid probability.\n",
        "        # Then, we use `nn.Unflatten` to reshape the output to the correct image size.\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(Z_dim, H_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H_dim, H_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H_dim, X_dim),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Unflatten(1, (1, height, width)),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        probs = self.model(z)\n",
        "        return D.Independent(D.Bernoulli(probs), 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH51kU58N1Xj"
      },
      "source": [
        "## VAE training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmtosD-GaMlT"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "\n",
        "prior = D.Independent(D.Normal(torch.zeros(Z_dim).to(device), torch.ones(Z_dim).to(device)), 1)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(device).float()\n",
        "        # ===== Forward pass =====\n",
        "        z_dist = encoder(x)\n",
        "        # TODO: Sample `z` using the reparameterization trick based on `z_dist.base_dist.loc` and `z_dist.base_dist.scale`.\n",
        "        # Alternatively, use `z_dist.rsample()`, which already implements the reparameterization trick.\n",
        "        z =\n",
        "        #z = torch.randn(z_dist.batch_shape + z_dist.event_shape).to(device) * z_dist.base_dist.scale + z_dist.base_dist.loc\n",
        "        x_dist = decoder(z)\n",
        "        # ===== Loss =====\n",
        "        # TODO: Compute the reconstruction loss, the complexity loss, and the total loss.\n",
        "        # For the reconstruction loss, use the `log_prob` method of `x_dist`.\n",
        "        # For the complexity loss, use the `D.kl_divergence` function, which allows to compute the KL divergence\n",
        "        # between two normal distributions.\n",
        "\n",
        "\n",
        "        loss =\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # ===== Logging =====\n",
        "        if step % 100 == 0:\n",
        "            print(f'epoch [{epoch + 1}/{num_epochs}], step {step + 1}, loss:{loss.item():.4f}')\n",
        "        step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_M3iyspN1Xk"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "Sampling is simply done by decoding a sample from the prior. The quality of samples can be improved by using a bigger model and training for more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBvHywUmaMlU"
      },
      "outputs": [],
      "source": [
        "z = prior.sample((batch_size,))\n",
        "with torch.no_grad():\n",
        "    # Instead of sampling from the Bernouilli distribution of the decoder,\n",
        "    # we decide to take the mode of the distribution, so that images are less noisy.\n",
        "    sample = decoder(z).mode\n",
        "plot_images(sample.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuclAe3saMlU"
      },
      "source": [
        "## Interpolation\n",
        "\n",
        "Images that are semantically similar will tend to be close to each other in the latent space.\n",
        "Thus, a linear interpolation between samples in the latent space will create an interpolation\n",
        "between images that are semantically similar in the image space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWYUhlb4aMlU"
      },
      "outputs": [],
      "source": [
        "z_list = []\n",
        "for _ in range(5):\n",
        "    z1, z2 = prior.sample(), prior.sample()\n",
        "    alpha = torch.linspace(0, 1, 16)[:, None]\n",
        "    z_interp = z1 * (1 - alpha) + z2 * alpha\n",
        "    z_list.append(z_interp)\n",
        "z = torch.cat(z_list)\n",
        "with torch.no_grad():\n",
        "    sample = decoder(z).mode\n",
        "plot_images(sample.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJsBsPHHN1Xk"
      },
      "source": [
        "# Optional experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npnIj9jZaMlU"
      },
      "source": [
        "## Dimensionality reduction\n",
        "\n",
        "Dimensionaly reduction techniques such t-SNE can be costly on high dimensional data such as images.\n",
        "One approach is to reduce the dimension using, e.g., a variational auto-encoder, and before applying t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETNgt3TuaMlU"
      },
      "outputs": [],
      "source": [
        "test_data = MNIST('./data', train=False, transform=transform)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "z_list = []\n",
        "y_list = []\n",
        "for x, y in test_loader:\n",
        "    x = x.to(device).float()\n",
        "    with torch.no_grad():\n",
        "        z_dist = encoder(x)\n",
        "    z_list.append(z_dist.base_dist.loc)\n",
        "    y_list.append(y)\n",
        "z = torch.cat(z_list, dim=0)[:2000]\n",
        "y = torch.cat(y_list, dim=0)[:2000]\n",
        "\n",
        "tsne = TSNE(n_components=2, learning_rate='auto', init='pca', random_state=0)\n",
        "tsne_results = tsne.fit_transform(z.cpu())\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=tsne_results[:, 0],\n",
        "    y=tsne_results[:, 1],\n",
        "    hue=y.numpy().astype('str'),\n",
        "    hue_order=[str(i) for i in range(10)],\n",
        "    alpha=0.3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXT4MFODN1Xk"
      },
      "source": [
        "## Convolutional variational auto-encoders\n",
        "\n",
        "In this lab, we implemented the encoders and decoders using fully connected neural networks for simplicity.\n",
        "Convolutional neural networks would be more suitable for images and would give improved results.\n",
        "Feel free to run the experiments using the encoders and decoders below.\n",
        "Better results could be achieved using more convolutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAKUlTzbaMlV"
      },
      "outputs": [],
      "source": [
        "base_channel_dim = 1 # The images are binary images with a single channel.\n",
        "C_dim = 32 # Number of channels in the first hidden layer of the encoder.\n",
        "kernel_size = 3\n",
        "stride = 2\n",
        "padding = 1\n",
        "\n",
        "def conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, **kwargs):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, **kwargs)\n",
        "\n",
        "def conv_transpose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, **kwargs):\n",
        "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, **kwargs)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            # base_channel_dim, 28, 28\n",
        "            conv2d(in_channels=base_channel_dim, out_channels=C_dim),\n",
        "            # C_dim, 14, 14\n",
        "            nn.ReLU(),\n",
        "            conv2d(in_channels=C_dim, out_channels=C_dim * 2),\n",
        "            # C_dim * 2, 7, 7\n",
        "            nn.ReLU(),\n",
        "            conv2d(in_channels=C_dim * 2, out_channels=C_dim * 4),\n",
        "            # C_dim * 4, 4, 4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            # C_dim * 4 * 4 * 4\n",
        "            nn.Linear(C_dim * 4 * 4 * 4, H_dim),\n",
        "            # H_dim\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H_dim, 2 * Z_dim),\n",
        "            # 2 * Z_dim\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We scale the values to the [-1, 1] interval.\n",
        "        x = (x - 0.5) * 2\n",
        "        mu, rho = torch.chunk(self.model(x), 2, dim=1)\n",
        "        return D.Independent(D.Normal(mu, F.softplus(rho)), 1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            # Z_dim\n",
        "            nn.Linear(Z_dim, H_dim),\n",
        "            # H_dim\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H_dim, C_dim * 4 * 4 * 4),\n",
        "            # C_dim * 4 * 4 * 4\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (C_dim * 4, 4, 4)),\n",
        "            # C_dim * 4, 4, 4\n",
        "            conv_transpose2d(in_channels=C_dim * 4, out_channels=C_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            # C_dim * 2, 7, 7\n",
        "            conv_transpose2d(in_channels=C_dim * 2, out_channels=C_dim, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            # C_dim, 14, 14\n",
        "            conv_transpose2d(in_channels=C_dim, out_channels=base_channel_dim, output_padding=1),\n",
        "            # 1, 28, 28\n",
        "            nn.Sigmoid(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z):\n",
        "        probs = self.model(z)\n",
        "        return D.Independent(D.Bernoulli(probs), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIvZTvOXN1Xl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
