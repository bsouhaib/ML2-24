{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssLI7a7GKzgB"
      },
      "source": [
        "# Machine Learning II 2023-2024 - UMONS \n",
        "# Generalization of neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIvAOzFjhtw6"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Import dataset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCMbdQa03Rxr"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed(10)\n",
        "    torch.cuda.manual_seed_all(10)\n",
        "else:\n",
        "    torch.device(\"cpu\")\n",
        "    torch.manual_seed(10)\n",
        "\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykv2pw97yWQU"
      },
      "source": [
        "## MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ8Gc1I3ZPBl"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "train_dataset=datasets.MNIST('data',train=True, transform=transform,download=True)\n",
        "test_dataset=datasets.MNIST('data',train=False, transform=transform,download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BqVTMFBb2UA"
      },
      "outputs": [],
      "source": [
        "# Create dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmW5Z1MVcJwR"
      },
      "outputs": [],
      "source": [
        "#Plot some examples\n",
        "fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
        "for i in range(25):\n",
        "    x, _ = train_dataset[i]\n",
        "    ax = axs[i // 5][i % 5]\n",
        "    ax.imshow(x.view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFWPz0pc9Aa3"
      },
      "outputs": [],
      "source": [
        "#Plot learning curve\n",
        "def plot_learning_curve(train_stat, test_stat, x, val=False, x_axis='Epochs'):\n",
        "  if val:\n",
        "      ln_label = [\"val acc.\", \"val loss\"]\n",
        "  else:\n",
        "      ln_label = [\"test acc.\", \"test loss\"]\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.title('Learning curve')\n",
        "  ln1 = ax.plot(x, train_stat['acc'], color='red', label='train acc.')\n",
        "  ln2 = ax.plot(x, test_stat['acc'], color='red', linestyle='--', label=ln_label[0])\n",
        "  ax.set_xlabel(x_axis)\n",
        "  ax.set_ylabel('Accuracy', color='red')\n",
        "  ax2 = ax.twinx()\n",
        "  ln3 = ax2.plot(x, train_stat['loss'], color='blue', label='train loss')\n",
        "  ln4 = ax2.plot(x, test_stat['loss'], color='blue', linestyle='--', label=ln_label[1])\n",
        "  ax2.set_ylabel('Loss', color=\"blue\")\n",
        "  lns = ln1+ln2+ln3+ln4\n",
        "  labs = [l.get_label() for l in lns]\n",
        "  ax.legend(lns, labs, loc=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNjVBvEZyomO"
      },
      "source": [
        "## Define Neural Net\n",
        "\n",
        "In forward pass input of a layer can be summed to its output `h = h + F.relu(self.fc(h))` which is called **residual connection** [[paper](https://arxiv.org/abs/1512.03385)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVCzc80Tx-3o"
      },
      "outputs": [],
      "source": [
        "class FCNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected neural network with residual connections in PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FCNet, self).__init__()\n",
        "\n",
        "        self.input_size = 784 # MNIST dataset images are of size (28,28)\n",
        "        self.hidden_size = 256\n",
        "        self.num_classes = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc4 = nn.Linear(self.hidden_size, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = h + F.relu(self.fc2(h))\n",
        "        h = h + F.relu(self.fc3(h))\n",
        "        return self.fc4(h)\n",
        "\n",
        "# Print network architecture using torchsummary\n",
        "summary(FCNet(), (784,), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PGWLMmodNjC"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, net, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Trains network for one epoch in batches.\n",
        "\n",
        "    Args:\n",
        "        train_loader: Data loader for training set.\n",
        "        net: Neural network model.\n",
        "        optimizer: Optimizer (e.g. SGD).\n",
        "        criterion: Loss function (e.g. cross-entropy loss).\n",
        "    \"\"\"\n",
        "\n",
        "    avg_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # iterate through batches\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.reshape(-1, 28*28).to(device)\n",
        "        labels= labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of loss and accuracy\n",
        "        avg_loss += loss\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return avg_loss/len(train_loader), 100 * correct / total\n",
        "\n",
        "def test(test_loader, net, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates network in batches.\n",
        "\n",
        "    Args:\n",
        "        test_loader: Data loader for test set.\n",
        "        net: Neural network model.\n",
        "        criterion: Loss function (e.g. cross-entropy loss).\n",
        "    \"\"\"\n",
        "\n",
        "    avg_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Use torch.no_grad to skip gradient calculation, not needed for evaluation\n",
        "    with torch.no_grad():\n",
        "        # iterate through batches\n",
        "        for data in test_loader:\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.reshape(-1, 28*28).to(device)\n",
        "            labels= labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # keep track of loss and accuracy\n",
        "            avg_loss += loss\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return avg_loss/len(test_loader), 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFfjWk8_g5QH"
      },
      "outputs": [],
      "source": [
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "\n",
        "train_stats = {}\n",
        "test_stats = {}\n",
        "\n",
        "# Create instance of Network\n",
        "net = FCNet().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=5e-1)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "    if epoch == 0:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      test_stats['loss']=[]\n",
        "      test_stats['acc']=[]\n",
        "\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "    # Test on data\n",
        "    test_loss, test_acc = test(test_loader,net,criterion)\n",
        "    test_stats['loss'].append(test_loss.cpu().detach().numpy())\n",
        "    test_stats['acc'].append(test_acc)\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtlC2-9VpbNi"
      },
      "outputs": [],
      "source": [
        "# Plot Learning curve\n",
        "plot_learning_curve(train_stats, test_stats, np.arange(0, epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MClpdNxjNIO"
      },
      "source": [
        "## Overfitting and the learning curve\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1024px-Overfitting.svg.png\" alt=\"drawing\" width=\"200\" align=\"right\" />\n",
        "\n",
        "**Overfitting** refers to the phenomenon when the model learns training data in detail along with noises to the extent that it degrades the performance of the model on new unseen data.\n",
        "\n",
        "**Overfitting** is illustrated in the image on the right: the green decision boundary is completely overfitted on the training samples, whereas the black decision boundary does not classify all training samples correctly but probably far more accurately models the true distribution of the data.\n",
        "\n",
        "A lot of factors influence the extent to which a model overfits to dataset - most importantly the size of the training set. The less samples a model can learn from, the more difficult it is to learn the true distribution of the data and the more the model will overfit. A **learning curve** shows the test accuracy or test loss of a model with respect to the number of training samples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyZQqMzSScch"
      },
      "outputs": [],
      "source": [
        "train_stats = {}\n",
        "test_stats = {}\n",
        "\n",
        "n_samples = [500, 1000, 2000, 3000, 4000]\n",
        "\n",
        "# Train model for different number of training samples\n",
        "for n in n_samples:\n",
        "    if n == 500:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      test_stats['loss']=[]\n",
        "      test_stats['acc']=[]\n",
        "\n",
        "    # Take subset of first n samples in training set\n",
        "    train_subset = torch.utils.data.Subset(train_dataset, range(n))\n",
        "    # Create new dataloader\n",
        "    train_loader = DataLoader(train_subset,batch_size=512)\n",
        "\n",
        "    # Create instance of Network\n",
        "    net = FCNet().to(device)\n",
        "\n",
        "    # Create loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=5e-1)\n",
        "\n",
        "    # Number of epochs to for training\n",
        "    epochs = 20\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "        # Train on data\n",
        "        train_loss, train_acc = train(train_loader, net, optimizer, criterion)\n",
        "\n",
        "        # Test on data\n",
        "        test_loss, test_acc = test(test_loader, net, criterion)\n",
        "\n",
        "    # Store accuracies\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "    test_stats['loss'].append(test_loss.cpu().detach().numpy())\n",
        "    test_stats['acc'].append(test_acc)\n",
        "\n",
        "\n",
        "# Plot learning curve\n",
        "plot_learning_curve(train_stats, test_stats, n_samples, x_axis = 'Number of samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH4K8v0OF5y4"
      },
      "source": [
        "**What is the observation?**\n",
        "\n",
        "The figure shows that the larger the training set, the higher the test accuracy and the lower the test loss of the model become. The training accuracy and training loss are a constant 100% and ~0, respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySJAZjzxAV-J"
      },
      "source": [
        "## Reduce the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrZXKBV9LKwW"
      },
      "outputs": [],
      "source": [
        "# Take subset of first 500 samples in training set\n",
        "train_subset = torch.utils.data.Subset(train_dataset, range(500))\n",
        "# Create new dataloader\n",
        "train_loader = DataLoader(train_subset,batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujNNTMdFNbEd"
      },
      "outputs": [],
      "source": [
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "train_stats = {}\n",
        "test_stats = {}\n",
        "\n",
        "# Create instance of Network\n",
        "net = FCNet().to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=5e-1)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "    if epoch == 0:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      test_stats['loss']=[]\n",
        "      test_stats['acc']=[]\n",
        "\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "    # Test on data\n",
        "    test_loss, test_acc = test(test_loader,net,criterion)\n",
        "    test_stats['loss'].append(test_loss.cpu().detach().numpy())\n",
        "    test_stats['acc'].append(test_acc)\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "748zysoFs8pY"
      },
      "outputs": [],
      "source": [
        "# Plot Learning curve\n",
        "plot_learning_curve(train_stats, test_stats, np.arange(0, epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNt1QNsqbpf"
      },
      "source": [
        "**What is the observation?**\n",
        "\n",
        "The training loss will continuously decrease, whereas the test loss will start to increase again at some point. Like the plateauing test accuracy, an increasing test loss is another indication of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDM4uIkWNybv"
      },
      "source": [
        "## L2 Regularization\n",
        "L2 regularization can be defined as:\n",
        "$$\\mathcal{L} = \\mathcal{L}_0 + \\frac{\\lambda}{2}\\sum_w w^2 $$\n",
        "Here, the total loss $\\mathcal{L}$ consists of the original task-specific loss $\\mathcal{L}_0$, e.g. cross-entropy, and the sum of the squared value of all network parameters $w$ (weights and biases), where $\\lambda$ specifies the relative weight of the two components. $\\lambda$ is a so-called hyper-parameter which needs to be specified before training, similar to the `epochs` parameter. This loss function thus encourages the network to learn small parameters, i.e. it minimizes the L2 norm of the parameters.\n",
        "\n",
        "For further reading you can refer to [online book](http://neuralnetworksanddeeplearning.com/chap3.html#why_does_regularization_help_reduce_overfitting).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V14N5iVcrAfj"
      },
      "source": [
        "In PyTorch, *weight decay*, which is simply another name for L2 regularization, is very conveniently built into the SGD optimizer. Have a look at the [docs](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) and initialize the optimizer with the proper settings to train the network using the PyTorch built-in weight decay functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBApWFhUxa6p"
      },
      "outputs": [],
      "source": [
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "train_stats = {}\n",
        "test_stats = {}\n",
        "\n",
        "# Create instance of Network\n",
        "net = FCNet().to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate=5e-1\n",
        "weight_decay=5e-3\n",
        "\n",
        "########################################################################\n",
        "#            TODO: Add L2 regularization the PyTorch way.              #\n",
        "########################################################################\n",
        "\n",
        "optimizer =\n",
        "\n",
        "########################################################################\n",
        "#                          END OF YOUR CODE                            #\n",
        "########################################################################\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "    if epoch == 0:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      test_stats['loss']=[]\n",
        "      test_stats['acc']=[]\n",
        "\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "    # Test on data\n",
        "    test_loss, test_acc = test(test_loader,net,criterion)\n",
        "    test_stats['loss'].append(test_loss.cpu().detach().numpy())\n",
        "    test_stats['acc'].append(test_acc)\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44f0R27TwAWd"
      },
      "outputs": [],
      "source": [
        "# Plot Learning curve\n",
        "plot_learning_curve(train_stats, test_stats, np.arange(0, epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vbivc1IBMXq"
      },
      "source": [
        "**What is the observation?**\n",
        "\n",
        "The test loss does not increase as much anymore, indicating that the network is overfitting less on the training set. Likewise, the test accuracy is higher for the regularized model. (However, due to the random nature of training deep neural networks this might not always be the case. Try training the model multiple times.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZT7cB_CVYL6"
      },
      "source": [
        "## Early stopping\n",
        "\n",
        "Instead of training a neural network for a fixed amount of epochs we can choose to halt training when the network starts to overfit on the training data. This is called **early stopping**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKhkuoakurQi"
      },
      "outputs": [],
      "source": [
        "# Make validation set - take samples 500-1500 from training set for convenience\n",
        "val_set = torch.utils.data.Subset(train_dataset, range(500,1500))\n",
        "# Create new dataloader\n",
        "val_loader = DataLoader(val_set,batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MiKvif8_VE"
      },
      "source": [
        "Now implement early stopping in the training loop. In short, the pseudo code for early stopping can be written as:\n",
        "```python\n",
        "for i in range(epochs):\n",
        "    train(model)\n",
        "    metric = validate(model)\n",
        "    if metric > metric_best:\n",
        "        metric_best = metric\n",
        "        reset patience_counter\n",
        "    else:\n",
        "        increment patience_counter\n",
        "        if patience_counter == patience:\n",
        "            halt training\n",
        "```\n",
        "\n",
        "What value for the `patience` parameter works best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z-7Ph5XuAnW"
      },
      "outputs": [],
      "source": [
        "# Create instance of Network\n",
        "net = FCNet().to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=5e-1, weight_decay=5e-3)\n",
        "\n",
        "# Set the number of epochs for training\n",
        "epochs = 20\n",
        "train_stats = {}\n",
        "val_stats = {}\n",
        "\n",
        "# Patience - how many epochs to keep training after accuracy has not improved\n",
        "patience = 0\n",
        "\n",
        "# Initialize early stopping variables\n",
        "val_acc_best = 0\n",
        "patience_cnt = 0\n",
        "final_epoch = 0\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "    final_epoch = epoch +1\n",
        "    if epoch == 0:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      val_stats['loss']=[]\n",
        "      val_stats['acc']=[]\n",
        "\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "\n",
        "    # Test on validation set\n",
        "    val_loss, val_acc = test(val_loader,net,criterion)\n",
        "    val_stats['loss'].append(val_loss.cpu().detach().numpy())\n",
        "    val_stats['acc'].append(val_acc)\n",
        "\n",
        "    ########################################################################\n",
        "    #                   TODO: Implement early stopping.                    #\n",
        "    #         Hint: use 'break' to break out of the training loop.         #\n",
        "    ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "# Test on test set\n",
        "test_loss, test_acc = test(test_loader,net,criterion)\n",
        "\n",
        "print('Finished Training')\n",
        "print('Validation accuracy:\\t{:.2f}'.format(val_acc))\n",
        "print('Test accuracy:\\t\\t{:.2f}'.format(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fScr2H6Gw9RE"
      },
      "outputs": [],
      "source": [
        "# Plot Learning curve\n",
        "plot_learning_curve(train_stats, val_stats, np.arange(0, final_epoch), val=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9nhiFVEHbVY"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "Dropout is a regularization method that modifies the network architecture itself rather than the way the network is optimized. A Dropout layer deactivates a random fraction $p$ of the neurons in a layer during a training iterations by setting them to zero. After performing the forward and backward pass for the current iteration, a different set of neurons is deactivated.\n",
        "\n",
        "You can refer to the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and implement some Dropout layers in the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ontWUwHe5V"
      },
      "outputs": [],
      "source": [
        "class FCNet_do(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple fully connected neural network with residual connections and dropout\n",
        "    layers in PyTorch. Layers are defined in __init__ and forward pass\n",
        "    implemented in forward.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FCNet_do, self).__init__()\n",
        "\n",
        "        self.input_size = 784\n",
        "        self.hidden_size = 256\n",
        "        self.num_classes = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc4 = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        p = 0.4\n",
        "\n",
        "        ########################################################################\n",
        "        #                     TODO: Define dropout layers.                     #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################################################################\n",
        "        #         TODO: Modify forward pass to include dropout layers.         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        return self.fc4(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAPlLrHqHe-w"
      },
      "outputs": [],
      "source": [
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "train_stats = {}\n",
        "test_stats = {}\n",
        "\n",
        "# Create instance of Network\n",
        "net = FCNet_do().to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=5e-1, weight_decay=5e-3)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "    if epoch == 0:\n",
        "      train_stats['loss']=[]\n",
        "      train_stats['acc']=[]\n",
        "      test_stats['loss']=[]\n",
        "      test_stats['acc']=[]\n",
        "\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "\n",
        "    train_stats['loss'].append(train_loss.cpu().detach().numpy())\n",
        "    train_stats['acc'].append(train_acc)\n",
        "    # Test on data\n",
        "    test_loss, test_acc = test(test_loader,net,criterion)\n",
        "    test_stats['loss'].append(test_loss.cpu().detach().numpy())\n",
        "    test_stats['acc'].append(test_acc)\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReEyFaCJx3YY"
      },
      "outputs": [],
      "source": [
        "# Plot Learning curve\n",
        "plot_learning_curve(train_stats, test_stats, np.arange(0, epochs))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
